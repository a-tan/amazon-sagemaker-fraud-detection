{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using Linear Learner and Inference Pipeline\n",
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. When an invocation request is made for a particular model, Amazon SageMaker routes the request to an instance assigned to that model, downloads the model artifacts from S3 onto that instance, and initiates loading of the model into the memory of the container. As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped and the invocation is performed immediately.\n",
    "\n",
    "Amazon SageMaker inference pipeline model consists of a sequence of containers that serve inference requests by combining preprocessing, predictions and post-processing data science tasks.  An inference pipeline allows you to apply the same preprocessing code used during model training, to process the inference request data used for predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/Hosting_RealTime_InfPipeline_MME.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/Hosting_RealTime_InfPipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/Hosting_RealTime_MME-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/Hosting_RealTime_MME-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To demonstrate how multi-model endpoints are created and used with inference pipeline, this notebook provides an example using a set of Linear Learner models that each predict housing prices for a single location. This domain is used as a simple example to easily experiment with multi-model endpoints.  \n",
    "\n",
    "This notebook showcases two MME capabilities: \n",
    "* Native MME support with Amazon SageMaker Linear Learner algorithm.  Because of the native support there is no need for you to create a custom container.  \n",
    "* Native MME support with Amazon SageMaker Inference Pipelines.\n",
    "\n",
    "\n",
    "To demonstrate these capabilities, the notebook discusses the use case of predicting house prices in multiple cities using linear regression.  House prices are predicted based on features like number of bedrooms, number of garages, square footage etc.  Depending on the city, the features affect the house price differently.  For example, small changes in the square footage cause a drastic change in house prices in New York when compared to price changes in Houston.  For accurate house price predictions, we will train multiple linear regression models, a unique location specific model per city.  \n",
    "\n",
    "It is also possible to provide Granular InvokeModel access to multiple models hosted on the MME using IAM condition key. For details of how to do it, please, refer to the full version of this notebook which could be found here:\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/multi_model_linear_learner_home_value/linear_learner_multi_model_endpoint_inf_pipeline.ipynb\n",
    "\n",
    "https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/multi_model_linear_learner_home_value/linear_learner_multi_model_endpoint_inf_pipeline.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Generate synthetic data for housing models](#Generate-synthetic-data-for-housing-models)\n",
    "1. [Preprocess-synthetic-housing-data-using-scikit-learn](#Preprocess-synthetic-housing-data-using-scikit-learn)\n",
    "1. [Create model entity with multi model support](#Create-sagemaker-multi-model-support)\n",
    "1. [Create an inference pipeline with sklearn model and MME linear learner model](#Create-inference-pipeline)\n",
    "1. [Exercise the inference pipeline - Get predictions from the different  linear learner models](#Exercise-inference-pipeline)\n",
    "1. [Update Multi Model Endpoint with new models](#update-models)\n",
    "1. [Clean up](#CleanUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 1 - Generate synthetic data for housing models <a id='Generate-synthetic-data-for-housing-models'></a>\n",
    "\n",
    "In this section, you will generate synthetic data that will be used to train the linear learner models.  The data generated consists of 6 numerical features - the year the house was built in, house size in square feet, number of bedrooms, number of bathroom, the lot size and number of garages and two categorial features - deck and front_porch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from random import choice\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sagemaker.multidatamodel import MULTI_MODEL_CONTAINER_MODE\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to generate random house price dataset.\n",
    "def gen_price(house):\n",
    "    \"\"\"Generate price based on features of the house\"\"\"\n",
    "\n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        garage = 1\n",
    "    else:\n",
    "        garage = 0\n",
    "\n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        front_porch = 1\n",
    "    else:\n",
    "        front_porch = 0\n",
    "\n",
    "    price = int(150 * house['SQUARE_FEET'] + \\\n",
    "                10000 * house['NUM_BEDROOMS'] + \\\n",
    "                15000 * house['NUM_BATHROOMS'] + \\\n",
    "                15000 * house['LOT_ACRES'] + \\\n",
    "                10000 * garage + \\\n",
    "                10000 * front_porch + \\\n",
    "                15000 * house['GARAGE_SPACES'] - \\\n",
    "                5000 * (MAX_YEAR - house['YEAR_BUILT']))\n",
    "    return price\n",
    "\n",
    "def gen_yes_no():\n",
    "    \"\"\"Generate values (y/n) for categorical features\"\"\"\n",
    "    answer = choice(['y', 'n'])\n",
    "    return answer\n",
    "\n",
    "def gen_random_house():\n",
    "    \"\"\"Generate a row of data (single house information)\"\"\"\n",
    "    house = {'SQUARE_FEET':    np.random.normal(3000, 750),\n",
    "             'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "             'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "             'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "             'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "             'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10))),\n",
    "             'FRONT_PORCH':   gen_yes_no(),\n",
    "             'DECK':          gen_yes_no()\n",
    "            }\n",
    "    \n",
    "    price = gen_price(house)\n",
    "    \n",
    "    return [house['YEAR_BUILT'],   \n",
    "            house['SQUARE_FEET'], \n",
    "            house['NUM_BEDROOMS'], \n",
    "            house['NUM_BATHROOMS'], \n",
    "            house['LOT_ACRES'],    \n",
    "            house['GARAGE_SPACES'],\n",
    "            house['FRONT_PORCH'],    \n",
    "            house['DECK'], \n",
    "            price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    \"\"\"Generate housing dataset\"\"\"\n",
    "    house_list = []\n",
    "\n",
    "    for _ in range(num_houses):\n",
    "        house_list.append(gen_random_house())\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        house_list,\n",
    "        columns=[\n",
    "            'YEAR_BUILT',\n",
    "            'SQUARE_FEET',\n",
    "            'NUM_BEDROOMS',\n",
    "            'NUM_BATHROOMS',\n",
    "            'LOT_ACRES',\n",
    "            'GARAGE_SPACES',\n",
    "            'FRONT_PORCH',\n",
    "            'DECK',\n",
    "            'PRICE']\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR_BUILT</th>\n",
       "      <th>SQUARE_FEET</th>\n",
       "      <th>NUM_BEDROOMS</th>\n",
       "      <th>NUM_BATHROOMS</th>\n",
       "      <th>LOT_ACRES</th>\n",
       "      <th>GARAGE_SPACES</th>\n",
       "      <th>FRONT_PORCH</th>\n",
       "      <th>DECK</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>4054.084574</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>670412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1989</td>\n",
       "      <td>2290.361147</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>311204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994</td>\n",
       "      <td>3615.299424</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>542344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993</td>\n",
       "      <td>3489.692094</td>\n",
       "      <td>4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>521353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>2830.288837</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>470993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR_BUILT  SQUARE_FEET  NUM_BEDROOMS  NUM_BATHROOMS  LOT_ACRES  \\\n",
       "0        2013  4054.084574             2            2.0       0.82   \n",
       "1        1989  2290.361147             5            2.5       1.01   \n",
       "2        1994  3615.299424             2            2.5       1.17   \n",
       "3        1993  3489.692094             4            2.5       1.36   \n",
       "4        2007  2830.288837             5            1.5       0.93   \n",
       "\n",
       "   GARAGE_SPACES FRONT_PORCH DECK   PRICE  \n",
       "0              2           n    n  670412  \n",
       "1              1           n    n  311204  \n",
       "2              2           y    n  542344  \n",
       "3              2           n    y  521353  \n",
       "4              0           y    n  470993  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate housing data for multiple locations.\n",
    "\n",
    "for loc in LOCATIONS[:10]:\n",
    "    houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "\n",
    "#Shows the first few lines of data.\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Preprocess the raw housing data using Scikit Learn <a id='Preprocess-synthetic-housing-data-using-scikit-learn'></a>\n",
    "\n",
    "In this section, the categorical features of the data (deck and porch) are pre-processed using sklearn to convert them to one hot encoding representation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET :  sagemaker-us-west-2-328296961357\n",
      "ROLE :  arn:aws:iam::328296961357:role/service-role/AmazonSageMaker-ExecutionRole-20191125T182032\n"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "BUCKET  = sagemaker_session.default_bucket()\n",
    "print(\"BUCKET : \", BUCKET)\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"ROLE : \", role)\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION = boto3.Session().region_name\n",
    "\n",
    "PREFIX = 'Pipeline_MME_demo'\n",
    "DATA_PREFIX = 'DEMO_MME_LINEAR_LEARNER'\n",
    "HOUSING_MODEL_NAME = 'housing'\n",
    "MULTI_MODEL_ARTIFACTS = 'multi_model_artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='linear-learner')\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-328296961357/Pipeline_MME_demo/model/SKLearn/model.tar.gz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## upload model data to S3\n",
    "SKLearn_model_uri = sagemaker_session.upload_data(\n",
    "    path='model/SKLearn/model.tar.gz',\n",
    "    bucket=BUCKET,\n",
    "    key_prefix='{}/model/SKLearn'.format(PREFIX)\n",
    ")\n",
    "SKLearn_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Create Sagemaker model with multi model support <a id='Create-sagemaker-multi-model-support'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_model_artifacts(model_data_url):\n",
    "    # extract the s3 key from the full url to the model artifacts\n",
    "    s3_key = model_data_url.split('s3://{}/'.format(BUCKET))[1]\n",
    "    # get the part of the key that identifies the model within the model artifacts folder\n",
    "    model_name_plus = s3_key[s3_key.find('model_artifacts') + len('model_artifacts') + 1:]\n",
    "    # finally, get the unique model name (e.g., \"NewYork_NY\")\n",
    "    model_name = re.findall('^(.*?)/', model_name_plus)[0]\n",
    "    return s3_key, model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the model artifacts from the original output of the training job to the place in\n",
    "# s3 where the multi model endpoint will dynamically load individual models\n",
    "def deploy_artifacts_to_mme(model_name):\n",
    "    print(\"model_name :\", model_name)\n",
    "    key = '{}/{}/{}'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, model_name)    \n",
    "    copy_source = 'model/LL/{}'.format(model_name)  \n",
    "    print('Copying {} model\\n   from: {}\\n     to: {}...'.format(model_name, copy_source, key))\n",
    "    sagemaker_session.upload_data(path=copy_source, bucket=BUCKET, key_prefix=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath='model/LL/'\n",
    "model_list=listdir(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old model artifacts from DEMO_MME_LINEAR_LEARNER/multi_model_artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': 'J0FXNGWX3V0XTYET',\n",
       "   'HostId': 'KoJILHi4NWvAOFBK7ba9aG3dai3zUMKnPVDwijloJ7N/GxkzQx7B0Jej9PRVhBVjKHOsU19jCiU=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'KoJILHi4NWvAOFBK7ba9aG3dai3zUMKnPVDwijloJ7N/GxkzQx7B0Jej9PRVhBVjKHOsU19jCiU=',\n",
       "    'x-amz-request-id': 'J0FXNGWX3V0XTYET',\n",
       "    'date': 'Sun, 30 Jan 2022 23:26:28 GMT',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3',\n",
       "    'connection': 'close'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/NewYork_NY/NewYork_NY.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX/Houston_TX.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/LosAngeles_CA/LosAngeles_CA.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Chicago_IL/Chicago_IL.tar.gz'}]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, clear out old versions of the model artifacts from previous runs of this notebook\n",
    "s3_bucket = s3.Bucket(BUCKET)\n",
    "full_input_prefix = '{}/multi_model_artifacts'.format(DATA_PREFIX)\n",
    "print('Removing old model artifacts from {}'.format(full_input_prefix))\n",
    "s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name : Houston_TX\n",
      "Copying Houston_TX model\n",
      "   from: model/LL/Houston_TX\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Houston_TX...\n",
      "model_name : Chicago_IL\n",
      "Copying Chicago_IL model\n",
      "   from: model/LL/Chicago_IL\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/Chicago_IL...\n",
      "model_name : LosAngeles_CA\n",
      "Copying LosAngeles_CA model\n",
      "   from: model/LL/LosAngeles_CA\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/LosAngeles_CA...\n"
     ]
    }
   ],
   "source": [
    "## Deploy all but the last model trained to MME\n",
    "## We will use the last model to show how to update an existing MME in Section 7\n",
    "for model in model_list[:-1]:\n",
    "    deploy_artifacts_to_mme(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "\n",
    "_model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "\n",
    "ll_multi_model = MultiDataModel(\n",
    "        name=MODEL_NAME,\n",
    "        model_data_prefix=_model_url,\n",
    "        image_uri=container,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "_model_url\n",
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : Create an inference pipeline with sklearn model and MME linear learner model <a id='Create-inference-pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the inference pipeline using the Pipeline Model API.  This sets up a list of models in a single endpoint; In this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "SKLearn_model_uri = 's3://sagemaker-us-west-2-328296961357/scikit-learn-preprocessor-2022-01-29-00-05-36/output/model.tar.gz' \n",
    "scikit_learn_inference_model = SKLearnModel(\n",
    "    role=role,\n",
    "    model_data=SKLearn_model_uri,\n",
    "    framework_version=\"0.20.0\",    \n",
    "    py_version=\"py3\",\n",
    "#    source_dir=\"code\",\n",
    "    entry_point=\"sklearn_preprocessor.py\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "model_name = '{}-{}'.format('inference-pipeline', timestamp_prefix)\n",
    "endpoint_name = '{}-{}'.format('inference-pipeline-ep', timestamp_prefix)\n",
    "\n",
    "sm_model = PipelineModel(\n",
    "    name=model_name, \n",
    "    role=role, \n",
    "    sagemaker_session=sagemaker_session,\n",
    "    models=[\n",
    "        scikit_learn_inference_model, \n",
    "        ll_multi_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 :  Exercise the inference pipeline - Get predictions from  different  linear learner models. <a id='Exercise-inference-pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Predictor\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "csv_serializer = sagemaker.serializers.CSVSerializer()\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_house_value(features, model_name, predictor_to_use):\n",
    "    print('Using model {} to predict price of this house: {}'.format(model_name,\n",
    "                                                                     features))\n",
    "    body = ','.join(map(str, features)) + '\\n'\n",
    "    start_time = time.time()\n",
    "     \n",
    "    response = predictor_to_use.predict(features, target_model=model_name)\n",
    "    \n",
    "    response_json = json.loads(response)\n",
    "        \n",
    "    predicted_value = response_json['predictions'][0]['score']    \n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value, int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX/Houston_TX.tar.gz to predict price of this house: [1972, 2109.3014242250642, 5, 1.0, 1.4, 2, 'y', 'n']\n",
      "$221,570.69, took 2,568 ms\n",
      "\n",
      "Using model Houston_TX/Houston_TX.tar.gz to predict price of this house: [2009, 2719.5126905496927, 6, 1.5, 0.88, 2, 'n', 'y']\n",
      "$481,461.19, took 56 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [1987, 3251.018458718614, 3, 2.0, 1.01, 0, 'n', 'y']\n",
      "$407,258.41, took 1,618 ms\n",
      "\n",
      "Using model Houston_TX/Houston_TX.tar.gz to predict price of this house: [2004, 2665.6648461374516, 3, 2.5, 0.97, 2, 'y', 'n']\n",
      "$452,122.09, took 48 ms\n",
      "\n",
      "Using model Chicago_IL/Chicago_IL.tar.gz to predict price of this house: [1978, 2685.346721775181, 2, 1.5, 0.7, 3, 'y', 'y']\n",
      "$316,825.41, took 43 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    model_name = model_list[np.random.randint(0, len(model_list) - 1)]\n",
    "    full_model_name = '{}/{}.tar.gz'.format(model_name,model_name)\n",
    "    predict_one_house_value(gen_random_house()[:-1], full_model_name, predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 - Add a new model to the endpoint by simply copying the model artifact to the S3 location and invoking it\n",
    "<a id='update-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name : NewYork_NY\n",
      "Copying NewYork_NY model\n",
      "   from: model/LL/NewYork_NY\n",
      "     to: DEMO_MME_LINEAR_LEARNER/multi_model_artifacts/NewYork_NY...\n"
     ]
    }
   ],
   "source": [
    "## Copy the last model\n",
    "deploy_artifacts_to_mme(model_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model NewYork_NY/NewYork_NY.tar.gz to predict price of this house: [1996, 2742.3434898920777, 4, 2.5, 0.88, 0, 'y', 'n']\n",
      "$404,989.22, took 1,611 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = model_list[-1]\n",
    "full_model_name = '{}/{}.tar.gz'.format(model_name,model_name)\n",
    "predict_one_house_value(gen_random_house()[:-1], full_model_name, predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up<a id='CleanUp'></a>\n",
    "Clean up the endpoint to avoid unneccessary costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the model and the endpoint\n",
    "predictor.delete_model() \n",
    "predictor.delete_endpoint() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
