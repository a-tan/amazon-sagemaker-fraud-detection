{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11dba3d",
   "metadata": {},
   "source": [
    "# Running multi-container endpoints on Amazon SageMaker\n",
    "\n",
    "SageMaker multi-container endpoints enable customers to deploy multiple containers to deploy different models on a SageMaker endpoint. The containers can be run in a sequence as an inference pipeline, or each container can be accessed individually by using direct invocation to improve endpoint utilization and optimize costs.\n",
    "\n",
    "\n",
    "This notebook shows how to create a multi-container endpoint which will host both the PyTorch(>=1.5) model and a TensorFlow(>=2.0) model, on a single endpoint. Here, `Direct` invocation behavior of multi-container endpoints is showcased where each model container can be invoked directly rather than being called in a sequence.\n",
    "\n",
    "This notebook is divided in the following sections:\n",
    "\n",
    "1. **Pre-requisites**\n",
    "1. **Train a TensorFlow Model in SageMaker**\n",
    "1. **Train a PyTorch Model in SageMaker**\n",
    "1. **Setup Multi-container Endpoint with Direct Invocation**\n",
    "1. **Inference**\n",
    "1. **Clean up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab2f9b",
   "metadata": {},
   "source": [
    "## Section 1: Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b6a04",
   "metadata": {},
   "source": [
    "First, import some necessary libraries and variables. This is the place where the output paths for the models are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57382cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.mnist import mnist_to_numpy, normalize\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "output_prefix = \"/multi-container-endpoint/output\"\n",
    "output_path = \"s3://\" + bucket + output_prefix\n",
    "\n",
    "region = sess.boto_region_name\n",
    "\n",
    "sm_client = sess.sagemaker_client\n",
    "runtime_sm_client = sess.sagemaker_runtime_client\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9830ab",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93999cf",
   "metadata": {},
   "source": [
    "This notebook uses `MNIST` dataset. `MNIST` is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled `28x28` pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58332dbb",
   "metadata": {},
   "source": [
    "### Set up channels for training and testing data\n",
    "\n",
    "Next, the framework Estimator needs to know where to find the training and testing data. It can be a link to an S3 bucket, or it can be a path in the local file system if local mode is used. For this notebook, download the `MNIST` data from a public S3 bucket and upload it to the default bucket created in the first cell. \n",
    "\n",
    "__NOTE: Local mode is not supported in Studio.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27d17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Download training and testing data from a public S3 bucket\n",
    "\n",
    "\n",
    "def download_from_s3(data_dir=\"/tmp/data\", train=True):\n",
    "    \"\"\"Download MNIST dataset and convert it to numpy array\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): directory to save the data\n",
    "        train (bool): download training set\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    if train:\n",
    "        images_file = \"train-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"train-labels-idx1-ubyte.gz\"\n",
    "    else:\n",
    "        images_file = \"t10k-images-idx3-ubyte.gz\"\n",
    "        labels_file = \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    with open(\"utils/config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # download objects\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = config[\"public_bucket\"]\n",
    "    for obj in [images_file, labels_file]:\n",
    "        key = os.path.join(\"datasets/image/MNIST\", obj)\n",
    "        dest = os.path.join(data_dir, obj)\n",
    "        if not os.path.exists(dest):\n",
    "            s3.download_file(bucket, key, dest)\n",
    "    return\n",
    "\n",
    "\n",
    "download_from_s3(\"/tmp/data\", True)\n",
    "download_from_s3(\"/tmp/data\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ee436",
   "metadata": {},
   "source": [
    "### Create channels for SageMaker Training\n",
    "\n",
    "The keys of the dictionary `channels` are parsed to the training image, and it creates the environment variable `SM_CHANNEL_<key name>`. \n",
    "\n",
    "In this example, `SM_CHANNEL_TRAINING` and `SM_CHANNEL_TESTING` are created in the training image (checkout how `tensorflow/train.py` or `pytorch/train.py` to learn how to access these variables). For more information, see: [SM_CHANNEL_{channel_name}](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089be38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to the default bucket\n",
    "\n",
    "dataset_prefix = \"multi-container-endpoint/dataset\"\n",
    "\n",
    "loc = sess.upload_data(path=\"/tmp/data\", bucket=bucket, key_prefix=dataset_prefix)\n",
    "\n",
    "channels = {\"training\": loc, \"testing\": loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b454c",
   "metadata": {},
   "source": [
    "\n",
    "Now all the pre-requisites are set up it is time to train the models. In the following section, a TensorFlow model is trained on the `MNIST` dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126f36b",
   "metadata": {},
   "source": [
    "## Section 2: Train a TensorFlow model in SageMaker using the TensorFlow Estimator\n",
    "\n",
    "The `TensorFlow` class allows to run a training script on SageMaker infrastructure in a containerized environment.  \n",
    "\n",
    "It needs the following parameters to set up the environment:\n",
    "\n",
    "- `entry_point`: A user defined python file to be used by the training container as the instructions for training. This file is further discussed in the next subsection\n",
    "\n",
    "- `role`: An IAM role to make AWS service requests\n",
    "\n",
    "- `instance_type`: The type of SageMaker instance to run the training script. \n",
    "\n",
    "- `model_dir`: S3 bucket URI where the checkpoint data and models can be exported to during training (default: None). To disable having `model_dir` passed to the training script, set `model_dir`=False\n",
    "\n",
    "- `instance_count`: The number of instances needed to run the training job. Multiple instances are needed for distributed training\n",
    "\n",
    "- `output_path`: S3 bucket URI to save training output (model artifacts and output files)\n",
    "\n",
    "- `framework_version`: The version of TensorFlow to use.\n",
    "\n",
    "- `py_version`: The python version to use\n",
    "\n",
    "For more information, see [the API reference](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d351ae8",
   "metadata": {},
   "source": [
    "### Implement the entry point for training\n",
    "\n",
    "The entry point for training is a python script that provides all the code for training a TensorFlow model. It is used by the SageMaker TensorFlow Estimator (`TensorFlow` class) as the entry point for running the training job.\n",
    "\n",
    "Under the hood, SageMaker TensorFlow Estimator downloads a docker image with runtime environments specified by the parameters you used to initiate the estimator class, and it injects the training script into the docker image to be used as the entry point to run the container.\n",
    "\n",
    "In the rest of the notebook, *training image* refers to the docker image specified by the Estimator and *training container* refers to the container that runs the training image. \n",
    "\n",
    "This means the training script is very similar to a training script that might run outside Amazon SageMaker, but it can access the useful environment variables provided by the training image. Checkout [the complete list of environment variables](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md) for a complete description of all environment variables your training script can access to. \n",
    "\n",
    "In this example, the training script at `tensorflow/code/train.py` is used as the entry point for the TensorFlow Estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c164ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgzip\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtraceback\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Model\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlayers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Conv2D, Dense, Flatten\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "\u001b[37m# Define the model object\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mSmallConv\u001b[39;49;00m(Model):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(SmallConv, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = Conv2D(\u001b[34m32\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.flatten = Flatten()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.d1 = Dense(\u001b[34m128\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.d2 = Dense(\u001b[34m10\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcall\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        x = \u001b[36mself\u001b[39;49;00m.conv1(x)\r\n",
      "        x = \u001b[36mself\u001b[39;49;00m.flatten(x)\r\n",
      "        x = \u001b[36mself\u001b[39;49;00m.d1(x)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.d2(x)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Decode and preprocess data\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_to_numpy\u001b[39;49;00m(data_dir, images_file, labels_file):\r\n",
      "    \u001b[33m\"\"\"Byte string to numpy arrays\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m gzip.open(os.path.join(data_dir, images_file), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        images = np.frombuffer(f.read(), np.uint8, offset=\u001b[34m16\u001b[39;49;00m).reshape(-\u001b[34m1\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mwith\u001b[39;49;00m gzip.open(os.path.join(data_dir, labels_file), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        labels = np.frombuffer(f.read(), np.uint8, offset=\u001b[34m8\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m (images, labels)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmnist_to_numpy\u001b[39;49;00m(data_dir, train):\r\n",
      "    \u001b[33m\"\"\"Load raw MNIST data into numpy array\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        data_dir (str): directory of MNIST raw data.\u001b[39;49;00m\r\n",
      "\u001b[33m            This argument can be accessed via SM_CHANNEL_TRAINING\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        train (bool): use training data\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m        tuple of images and labels as numpy array\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m train:\r\n",
      "        images_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        labels_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        images_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        labels_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m convert_to_numpy(data_dir, images_file, labels_file)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mnormalize\u001b[39;49;00m(x, axis):\r\n",
      "    eps = np.finfo(\u001b[36mfloat\u001b[39;49;00m).eps\r\n",
      "\r\n",
      "    mean = np.mean(x, axis=axis, keepdims=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    \u001b[37m# avoid division by zero\u001b[39;49;00m\r\n",
      "    std = np.std(x, axis=axis, keepdims=\u001b[34mTrue\u001b[39;49;00m) + eps\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x - mean) / std\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Training logic\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\r\n",
      "    \u001b[37m# create data loader from the train / test channels\u001b[39;49;00m\r\n",
      "    x_train, y_train = mnist_to_numpy(data_dir=args.train, train=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    x_test, y_test = mnist_to_numpy(data_dir=args.test, train=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "    x_train, x_test = x_train.astype(np.float32), x_test.astype(np.float32)\r\n",
      "\r\n",
      "    \u001b[37m# normalize the inputs to mean 0 and std 1\u001b[39;49;00m\r\n",
      "    x_train, x_test = normalize(x_train, (\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m)), normalize(x_test, (\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[37m# expand channel axis\u001b[39;49;00m\r\n",
      "    \u001b[37m# tf uses depth minor convention\u001b[39;49;00m\r\n",
      "    x_train, x_test = np.expand_dims(x_train, axis=\u001b[34m3\u001b[39;49;00m), np.expand_dims(x_test, axis=\u001b[34m3\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# normalize the data to mean 0 and std 1\u001b[39;49;00m\r\n",
      "    train_loader = (\r\n",
      "        tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n",
      "        .shuffle(\u001b[36mlen\u001b[39;49;00m(x_train))\r\n",
      "        .batch(args.batch_size)\r\n",
      "    )\r\n",
      "\r\n",
      "    test_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(args.batch_size)\r\n",
      "\r\n",
      "    model = SmallConv()\r\n",
      "    model.compile()\r\n",
      "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    optimizer = tf.keras.optimizers.Adam(\r\n",
      "        learning_rate=args.learning_rate, beta_1=args.beta_1, beta_2=args.beta_2\r\n",
      "    )\r\n",
      "\r\n",
      "    train_loss = tf.keras.metrics.Mean(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_accuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    test_loss = tf.keras.metrics.Mean(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_accuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[90m@tf\u001b[39;49;00m.function\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_step\u001b[39;49;00m(images, labels):\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.GradientTape() \u001b[34mas\u001b[39;49;00m tape:\r\n",
      "            predictions = model(images, training=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "            loss = loss_fn(labels, predictions)\r\n",
      "        grad = tape.gradient(loss, model.trainable_variables)\r\n",
      "        optimizer.apply_gradients(\u001b[36mzip\u001b[39;49;00m(grad, model.trainable_variables))\r\n",
      "\r\n",
      "        train_loss(loss)\r\n",
      "        train_accuracy(labels, predictions)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[90m@tf\u001b[39;49;00m.function\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtest_step\u001b[39;49;00m(images, labels):\r\n",
      "        predictions = model(images, training=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "        t_loss = loss_fn(labels, predictions)\r\n",
      "        test_loss(t_loss)\r\n",
      "        test_accuracy(labels, predictions)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining starts ...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(args.epochs):\r\n",
      "        train_loss.reset_states()\r\n",
      "        train_accuracy.reset_states()\r\n",
      "        test_loss.reset_states()\r\n",
      "        test_accuracy.reset_states()\r\n",
      "\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch, (images, labels) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\r\n",
      "            train_step(images, labels)\r\n",
      "\r\n",
      "        \u001b[34mfor\u001b[39;49;00m images, labels \u001b[35min\u001b[39;49;00m test_loader:\r\n",
      "            test_step(images, labels)\r\n",
      "\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\r\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch + \u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mLoss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_loss.result()\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAccuracy: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_accuracy.result() * \u001b[34m100\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTest Loss: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_loss.result()\u001b[33m}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTest Accuracy: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_accuracy.result() * \u001b[34m100\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        )\r\n",
      "\r\n",
      "    \u001b[37m# Save the model\u001b[39;49;00m\r\n",
      "    \u001b[37m# A version number is needed for the serving container\u001b[39;49;00m\r\n",
      "    \u001b[37m# to load the model\u001b[39;49;00m\r\n",
      "    version = \u001b[33m\"\u001b[39;49;00m\u001b[33m00000000\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    ckpt_dir = os.path.join(args.model_dir, version)\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(ckpt_dir):\r\n",
      "        os.makedirs(ckpt_dir)\r\n",
      "    model.save(ckpt_dir)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m1e-3\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--beta_1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--beta_2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.999\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Environment variables given by the training image\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TESTING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args = parse_args()\r\n",
      "    train(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'tensorflow/code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fb1e5",
   "metadata": {},
   "source": [
    "### Set hyperparameters\n",
    "\n",
    "In addition, TensorFlow Estimator allows parsing command line arguments to your training script via `hyperparameters`. Note that TensorFlow 2.3.1 version is used for training, the same should be used for inference to avoid any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5734c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_output_path = output_path + \"/tensorflow\"\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"tensorflow/code\",  # directory of training script\n",
    "    role=role,\n",
    "    framework_version=\"2.3.1\",\n",
    "    model_dir=False,  # don't pass --model_dir to training script\n",
    "    py_version=\"py37\",\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=tf_output_path,\n",
    "    hyperparameters={\n",
    "        \"batch-size\": 512,\n",
    "        \"epochs\": 1,\n",
    "        \"learning-rate\": 1e-3,\n",
    "        \"beta_1\": 0.9,\n",
    "        \"beta_2\": 0.999,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66897e2f",
   "metadata": {},
   "source": [
    "### Run the training script on SageMaker\n",
    "Now, the TensorFlow training container has everything to execute the training script, model training can be started by calling `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857cb55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 20:04:17 Starting - Starting the training job...\n",
      "2022-01-28 20:04:43 Starting - Launching requested ML instancesProfilerReport-1643400256: InProgress\n",
      "...\n",
      "2022-01-28 20:05:17 Starting - Preparing the instances for training.........\n",
      "2022-01-28 20:06:44 Downloading - Downloading input data...\n",
      "2022-01-28 20:07:04 Training - Downloading the training image..\u001b[34m2022-01-28 20:07:24.705733: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:24.717135: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:25.104093: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:28,963 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:28,969 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:29,394 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:29,410 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:29,424 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:29,437 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 512,\n",
      "        \"beta_1\": 0.9,\n",
      "        \"beta_2\": 0.999,\n",
      "        \"learning-rate\": 0.001,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2022-01-28-20-04-16-983\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-328296961357/tensorflow-training-2022-01-28-20-04-16-983/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":512,\"beta_1\":0.9,\"beta_2\":0.999,\"epochs\":1,\"learning-rate\":0.001}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-328296961357/tensorflow-training-2022-01-28-20-04-16-983/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":512,\"beta_1\":0.9,\"beta_2\":0.999,\"epochs\":1,\"learning-rate\":0.001},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2022-01-28-20-04-16-983\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-328296961357/tensorflow-training-2022-01-28-20-04-16-983/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"512\",\"--beta_1\",\"0.9\",\"--beta_2\",\"0.999\",\"--epochs\",\"1\",\"--learning-rate\",\"0.001\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=512\u001b[0m\n",
      "\u001b[34mSM_HP_BETA_1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_BETA_2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 train.py --batch-size 512 --beta_1 0.9 --beta_2 0.999 --epochs 1 --learning-rate 0.001\u001b[0m\n",
      "\u001b[34mTraining starts ...\u001b[0m\n",
      "\u001b[34m[2022-01-28 20:07:32.942 ip-10-0-239-106.us-west-2.compute.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-01-28 20:07:33.241 ip-10-0-239-106.us-west-2.compute.internal:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-01-28 20:07:33.242 ip-10-0-239-106.us-west-2.compute.internal:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-01-28 20:07:33.242 ip-10-0-239-106.us-west-2.compute.internal:26 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-01-28 20:07:33.243 ip-10-0-239-106.us-west-2.compute.internal:26 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-01-28 20:07:33.243 ip-10-0-239-106.us-west-2.compute.internal:26 INFO state_store.py:75] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-01-28 20:07:33.243 ip-10-0-239-106.us-west-2.compute.internal:26 INFO hook.py:413] Monitoring the collections: losses, metrics, sm_metrics\u001b[0m\n",
      "\n",
      "2022-01-28 20:07:44 Training - Training image download completed. Training in progress.\u001b[34mEpoch 1, Loss: 0.2795651853084564, Accuracy: 91.53500366210938, Test Loss: 0.11100666224956512, Test Accuracy: 96.88999938964844\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:29.763744: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:29.763878: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:29.790840: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:49.453378: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/00000000/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/00000000/assets\u001b[0m\n",
      "\u001b[34m2022-01-28 20:07:50,045 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-01-28 20:08:04 Uploading - Uploading generated training model\n",
      "2022-01-28 20:08:04 Completed - Training job completed\n",
      "Training seconds: 84\n",
      "Billable seconds: 84\n"
     ]
    }
   ],
   "source": [
    "tf_estimator.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a753a86",
   "metadata": {},
   "source": [
    "### Inspect and store model data\n",
    "\n",
    "Now, the training is finished, the model artifact has been saved in the `output_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09888f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact saved at:\n",
      " s3://sagemaker-us-west-2-328296961357/multi-container-endpoint/output/tensorflow/tensorflow-training-2022-01-28-20-04-16-983/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "tf_mnist_model_data = tf_estimator.model_data\n",
    "print(\"Model artifact saved at:\\n\", tf_mnist_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bab855bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Downloader.download(tf_mnist_model_data, \"model/tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31870fdb",
   "metadata": {},
   "source": [
    "## Section 3: Train a PyTorch model in SageMaker using PyTorch Estimator\n",
    "\n",
    "In this section, A PyTorch model is trained on the same `MNIST` dataset. \n",
    "\n",
    "### PyTorch Estimator\n",
    "\n",
    "The `PyTorch` class allows to run the training script on SageMaker infrastructure in a containerized environment.\n",
    "\n",
    "It needs to have the following parameters to set up the environment:\n",
    "\n",
    "- `entry_point`: A user defined python file to be used by the training container as the instructions for training. This file is further discussed in the next subsection.\n",
    "\n",
    "- `role`: An IAM role to make AWS service requests\n",
    "\n",
    "- `instance_type`: The type of SageMaker instance to run the training script. \n",
    "\n",
    "- `instance_count`: The number of instances needed to run the training job. Multiple instances are needed for distributed training.\n",
    "\n",
    "- `output_path`: S3 bucket URI to save training output (model artifacts and output files)\n",
    "\n",
    "- `framework_version`: The version of PyTorch to use.\n",
    "\n",
    "- `py_version`: The python version to use\n",
    "\n",
    "For more information, see [the API reference](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html)\n",
    "\n",
    "\n",
    "### Implement the entry point for training\n",
    "\n",
    "The entry point for training is a python script that provides all the code for training a PyTorch model. It is used by the SageMaker PyTorch Estimator (`PyTorch` class above) as the entry point for running the training job.\n",
    "\n",
    "Under the hood, SageMaker PyTorch Estimator creates a docker image with runtime environments specified by the parameters used to initiate the Estimator class, and it injects the training script into the docker image to be used as the entry point to run the container. Here as well, the training script can access all the useful environment variables provided by the training image as described in **Section 2**. The training script present at `pytorch/code/train.py` is used as the entry point for the PyTorch Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5b69d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgzip\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataLoader, Dataset\r\n",
      "\r\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\r\n",
      "logger.setLevel(logging.DEBUG)\r\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\r\n",
      "\r\n",
      "\u001b[37m# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;49;00m\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m10\u001b[39;49;00m, \u001b[34m20\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2_drop = nn.Dropout2d()\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m320\u001b[39;49;00m, \u001b[34m50\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m50\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv1(x), \u001b[34m2\u001b[39;49;00m))\r\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv2_drop(\u001b[36mself\u001b[39;49;00m.conv2(x)), \u001b[34m2\u001b[39;49;00m))\r\n",
      "        x = x.view(-\u001b[34m1\u001b[39;49;00m, \u001b[34m320\u001b[39;49;00m)\r\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\r\n",
      "        x = F.dropout(x, training=\u001b[36mself\u001b[39;49;00m.training)\r\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Decode binary data from SM_CHANNEL_TRAINING\u001b[39;49;00m\r\n",
      "\u001b[37m# Decode and preprocess data\u001b[39;49;00m\r\n",
      "\u001b[37m# Create map dataset\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mnormalize\u001b[39;49;00m(x, axis):\r\n",
      "    eps = np.finfo(\u001b[36mfloat\u001b[39;49;00m).eps\r\n",
      "    mean = np.mean(x, axis=axis, keepdims=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    \u001b[37m# avoid division by zero\u001b[39;49;00m\r\n",
      "    std = np.std(x, axis=axis, keepdims=\u001b[34mTrue\u001b[39;49;00m) + eps\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x - mean) / std\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_to_tensor\u001b[39;49;00m(data_dir, images_file, labels_file):\r\n",
      "    \u001b[33m\"\"\"Byte string to torch tensor\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m gzip.open(os.path.join(data_dir, images_file), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        images = np.frombuffer(f.read(), np.uint8, offset=\u001b[34m16\u001b[39;49;00m).reshape(-\u001b[34m1\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m).astype(np.float32)\r\n",
      "\r\n",
      "    \u001b[34mwith\u001b[39;49;00m gzip.open(os.path.join(data_dir, labels_file), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        labels = np.frombuffer(f.read(), np.uint8, offset=\u001b[34m8\u001b[39;49;00m).astype(np.int64)\r\n",
      "\r\n",
      "    \u001b[37m# normalize the images\u001b[39;49;00m\r\n",
      "    images = normalize(images, axis=(\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[37m# add channel dimension (depth-major)\u001b[39;49;00m\r\n",
      "    images = np.expand_dims(images, axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# to torch tensor\u001b[39;49;00m\r\n",
      "    images = torch.tensor(images, dtype=torch.float32)\r\n",
      "    labels = torch.tensor(labels, dtype=torch.int64)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m images, labels\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMNIST\u001b[39;49;00m(Dataset):\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data_dir, train=\u001b[34mTrue\u001b[39;49;00m):\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m train:\r\n",
      "            images_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            labels_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            images_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            labels_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mt10k-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.images, \u001b[36mself\u001b[39;49;00m.labels = convert_to_tensor(data_dir, images_file, labels_file)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.labels)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.images[idx], \u001b[36mself\u001b[39;49;00m.labels[idx]\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\r\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\r\n",
      "        torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "    train_loader = DataLoader(\r\n",
      "        MNIST(args.train, train=\u001b[34mTrue\u001b[39;49;00m), batch_size=args.batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    test_loader = DataLoader(\r\n",
      "        MNIST(args.test, train=\u001b[34mFalse\u001b[39;49;00m), batch_size=args.test_batch_size, shuffle=\u001b[34mFalse\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "    net = Net().to(device)\r\n",
      "    loss_fn = nn.CrossEntropyLoss()\r\n",
      "    optimizer = optim.Adam(\r\n",
      "        net.parameters(), betas=(args.beta_1, args.beta_2), weight_decay=args.weight_decay\r\n",
      "    )\r\n",
      "\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mStart training ...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        net.train()\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (imgs, labels) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\r\n",
      "            imgs, labels = imgs.to(device), labels.to(device)\r\n",
      "            output = net(imgs)\r\n",
      "            loss = loss_fn(output, labels)\r\n",
      "\r\n",
      "            optimizer.zero_grad()\r\n",
      "            loss.backward()\r\n",
      "            optimizer.step()\r\n",
      "\r\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\r\n",
      "                \u001b[36mprint\u001b[39;49;00m(\r\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                        epoch,\r\n",
      "                        batch_idx * \u001b[36mlen\u001b[39;49;00m(imgs),\r\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\r\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader),\r\n",
      "                        loss.item(),\r\n",
      "                    )\r\n",
      "                )\r\n",
      "\r\n",
      "        \u001b[37m# test the model\u001b[39;49;00m\r\n",
      "        test(net, test_loader, device)\r\n",
      "\r\n",
      "    \u001b[37m# save model checkpoint\u001b[39;49;00m\r\n",
      "    save_model(net, args.model_dir)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\r\n",
      "    model.eval()\r\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        \u001b[34mfor\u001b[39;49;00m imgs, labels \u001b[35min\u001b[39;49;00m test_loader:\r\n",
      "            imgs, labels = imgs.to(device), labels.to(device)\r\n",
      "            output = model(imgs)\r\n",
      "            test_loss += F.cross_entropy(output, labels, reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).item()\r\n",
      "\r\n",
      "            pred = output.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m]\r\n",
      "            correct += pred.eq(labels.view_as(pred)).sum().item()\r\n",
      "\r\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\r\n",
      "    logger.info(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "            test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset), \u001b[34m100.0\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\r\n",
      "        )\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\r\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    torch.save(model.cpu().state_dict(), path)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m1000\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m0.001\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--beta_1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mBETA1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbeta1 (default: 0.9)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--beta_2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.999\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mBETA2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbeta2 (default: 0.999)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--weight-decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m1e-4\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mWD\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mL2 weight decay (default: 1e-4)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "        default=\u001b[34m100\u001b[39;49;00m,\r\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\r\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "\r\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TESTING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    args = parse_args()\r\n",
      "    train(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'pytorch/code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8ba74",
   "metadata": {},
   "source": [
    "### Set hyperparameters\n",
    "\n",
    "In addition, PyTorch Estimator allows parsing command line arguments to your training script via `hyperparameters`. Note that PyTorch 1.8.1 version is used for training, the same should be used for inference as well to avoid any errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555f2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_est = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"pytorch/code\",  # directory of your training script\n",
    "    role=role,\n",
    "    framework_version=\"1.8.1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=output_path + \"/pytorch\",\n",
    "    hyperparameters={\"batch-size\": 128, \"epochs\": 1, \"learning-rate\": 1e-3, \"log-interval\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e805e",
   "metadata": {},
   "source": [
    "### Run the training script on SageMaker\n",
    "Now, the PyTorch training container has everything to execute the training script. The training can be started by calling `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0321696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 21:21:13 Starting - Starting the training job...\n",
      "2022-01-28 21:21:37 Starting - Launching requested ML instancesProfilerReport-1643404872: InProgress\n",
      "...\n",
      "2022-01-28 21:22:11 Starting - Preparing the instances for training............\n",
      "2022-01-28 21:24:06 Downloading - Downloading input data...\n",
      "2022-01-28 21:24:38 Training - Downloading the training image...\n",
      "2022-01-28 21:25:01 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:02,503 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:02,506 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:02,516 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:05,544 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:06,047 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:06,066 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:06,083 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:06,095 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 128,\n",
      "        \"log-interval\": 100,\n",
      "        \"learning-rate\": 0.001,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-01-28-21-21-12-840\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-328296961357/pytorch-training-2022-01-28-21-21-12-840/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":128,\"epochs\":1,\"learning-rate\":0.001,\"log-interval\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-328296961357/pytorch-training-2022-01-28-21-21-12-840/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":128,\"epochs\":1,\"learning-rate\":0.001,\"log-interval\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-01-28-21-21-12-840\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-328296961357/pytorch-training-2022-01-28-21-21-12-840/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"128\",\"--epochs\",\"1\",\"--learning-rate\",\"0.001\",\"--log-interval\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch-size 128 --epochs 1 --learning-rate 0.001 --log-interval 100\u001b[0m\n",
      "\u001b[34mStart training ...\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:08.811 algo-1:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.310 algo-1:26 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.311 algo-1:26 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.311 algo-1:26 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.312 algo-1:26 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.312 algo-1:26 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.333 algo-1:26 INFO hook.py:591] name:conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:591] name:conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:591] name:conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:591] name:conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:591] name:fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:591] name:fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:591] name:fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:591] name:fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:593] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.334 algo-1:26 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-01-28 21:25:09.337 algo-1:26 INFO hook.py:488] Hook is writing from the hook with pid: 26\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)] Loss: 0.562919\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.560365\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.290681\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.349515\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1164, Accuracy: 9640/10000, 96.4)\u001b[0m\n",
      "\u001b[34mSaving the model\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1164, Accuracy: 9640/10000, 96.4)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model\u001b[0m\n",
      "\u001b[34m2022-01-28 21:25:20,482 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-01-28 21:25:38 Uploading - Uploading generated training model\n",
      "2022-01-28 21:25:38 Completed - Training job completed\n",
      "Training seconds: 84\n",
      "Billable seconds: 84\n"
     ]
    }
   ],
   "source": [
    "pytorch_est.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf743c",
   "metadata": {},
   "source": [
    "### Inspect and store model data\n",
    "\n",
    "Now, the training is finished, the model artifact has been saved in the `output_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "276a179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact saved at:\n",
      " s3://sagemaker-us-west-2-328296961357/multi-container-endpoint/output/pytorch/pytorch-training-2022-01-28-21-21-12-840/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "pt_mnist_model_data = pytorch_est.model_data\n",
    "print(\"Model artifact saved at:\\n\", pt_mnist_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b49c798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-328296961357/multi-container-endpoint/output/tensorflow/model.tar.gz'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload tensorflow model.tar.gz to s3\n",
    "updated_tf_model_key = \"multi-container-endpoint/output/tensorflow\"\n",
    "tf_updated_model_uri = S3Uploader.upload(\n",
    "    \"model/tensorflow/model.tar.gz\", \"s3://{}/{}\".format(bucket, updated_pt_model_key))\n",
    "tf_updated_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a02bf",
   "metadata": {},
   "source": [
    "\n",
    "## Section 4: Set up Multi-container endpoint with Direct Invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96a4df",
   "metadata": {},
   "source": [
    "In this section, a multi-container endpoint is set up.\n",
    "\n",
    "SageMaker multi-container endpoints enable customers to deploy multiple containers to deploy different models on the same SageMaker endpoint. The containers can be run in a sequence as an inference pipeline, or each container can be accessed individually by using `direct` invocation to improve endpoint utilization and optimize costs.\n",
    "\n",
    "The TensorFlow and PyTorch models, trained in the earlier sections would be deployed against a single sagemaker endpoint using multi-container capability of SageMaker Endpoints. This section uses`boto3` APIs.\n",
    "\n",
    "Setting up a multi-container endpoint is a multi-step process, which looks like the following:\n",
    "- Create inference container definitions for all the containers needed to deploy\n",
    "- Create a SageMaker model using the `create_model` API. Use the `Containers` parameter instead of `PrimaryContainer`, and include more than one container in the `Containers` parameter.\n",
    "- Create a SageMaker Endpoint Configuration using the `create_endpoint_config` API\n",
    "- Create a SageMaker Endpoint using the `create_endpoint` API which uses the model and endpoint configuration created in the earlier steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b0193",
   "metadata": {},
   "source": [
    "### Create inference container definition for TensorFlow model\n",
    "\n",
    "To create a container definition, following must be defined :\n",
    "\n",
    "- `ContainerHostname`: The value of the parameter uniquely identifies the container for the purposes of logging and metrics. The `ContainerHostname` parameter is required for each container in a multi-container endpoint with `direct` invocation. Though it can be skipped, in case of serial inference pipeline as the inference pipeline will assign a unique name automatically.\n",
    "\n",
    "- `Image`: It is the path where inference code is stored. This can be either in Amazon EC2 Container Registry or in a Docker registry that is accessible from the same VPC that is configured for the endpoint. If custom algorithm is used instead of an algorithm provided by Amazon SageMaker, the inference code must meet Amazon SageMaker requirements.\n",
    "\n",
    "- `ModelDataUrl`: The S3 path where the model artifacts, which result from model training, are stored. This path must point to a single GZIP compressed tar archive (`.tar.gz` suffix). The S3 path is required for Amazon SageMaker built-in algorithms/frameworks, but not if a custom algorithm (not provided by sagemaker) is used.\n",
    "\n",
    "For the Image argument, supply the ECR path of the TensorFlow 2.3.1 inference image. For deep learning images available in SageMaker, refer to [Available Deep Learning Containers Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e6ca7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ecr_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.3.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "tensorflow_container = {\n",
    "    \"ContainerHostname\": \"tensorflow-mnist\",\n",
    "    \"Image\": tf_ecr_image_uri,\n",
    "    \"ModelDataUrl\": tf_mnist_model_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d615ae",
   "metadata": {},
   "source": [
    "### Create inference container definition for PyTorch model\n",
    "\n",
    "Now similarly, create the container definition for PyTorch model. \n",
    "\n",
    "Here in addition to the arguments defined for TensorFlow container, one more additional argument needs to be defined which is `Environment`. This is because, the PyTorch model server needs to know how to load the model and make the predictions. This is explained in detail in the following section.\n",
    "\n",
    "\n",
    "To tell the inference image how to load the model checkpoint, it needs to implement:\n",
    "\n",
    "- How to parse the incoming request\n",
    "- How to use the trained model to make inference\n",
    "- How to return the prediction to the caller of the service\n",
    "\n",
    "\n",
    "To achieve this, it needs to:\n",
    "\n",
    "- implement a function called `model_fn` which returns a PyTorch model.\n",
    "- implement a function called `input_fn` function which handles data decoding and returns an object that can be passed to `predict_fn`\n",
    "- implement a function called `predict_fn` function which will perform the prediction and returns as object that can be passed to `output_fn`\n",
    "- implement a function called `output_fn` function which will perform the de-serialization of the output given by `predict_fn`\n",
    "\n",
    "\n",
    "To achieve this, `inference.py` is created which provides the implementation of all the above functions in that file. This file must be supplied as an environment variable `SAGEMAKER_PROGRAM`.\n",
    "\n",
    "The model and `inference.py` also need to be wrapped together in a single `tar.gz`. The following steps are performed to zip the inference and model file together:\n",
    "\n",
    "- Download the `model.tar.gz` containing the trained PyTorch model\n",
    "- Unzip the `model.tar.gz`. The `model.pth` file is visible after unzipping.\n",
    "- GZIP the `model file(.pth)` and the `inference.py` together in a new `tar.gz`\n",
    "- Upload the new `tar.gz` to `s3` location, to be referred in the `model container definition` later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67b708c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3Downloader.download(pt_mnist_model_data, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f671c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/direct-invocation_2022-01-28\n",
      "/home/ec2-user/SageMaker/direct-invocation_2022-01-28\n",
      "model.pth\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!cp model/pytorch/pt_model.tar.gz .\n",
    "!pwd\n",
    "!tar -xvf pt_model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4bbf2a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pth\n",
      "model.pth\n",
      "inference.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-328296961357/multi-container-endpoint/output/pytorch/updated/model.tar.gz'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move pytorch model to the working directory\n",
    "!cp model/pytorch/pt_model.tar.gz .\n",
    "\n",
    "# unzip the tar.gz\n",
    "!tar -xvf pt_model.tar.gz\n",
    "\n",
    "# after unzipping, remove the model.tar.gz\n",
    "!rm pt_model.tar.gz\n",
    "\n",
    "# copy the pytorch inference script to current dir\n",
    "!cp pytorch/code/inference.py .\n",
    "\n",
    "# gzip the inference.py and model file together in a new model.tar.gz\n",
    "!tar -czvf model.tar.gz model.pth inference.py\n",
    "\n",
    "# remove the residual files\n",
    "!rm inference.py model.pth\n",
    "\n",
    "# upload the new tar.gz to s3\n",
    "updated_pt_model_key = \"multi-container-endpoint/output/pytorch/updated\"\n",
    "pt_updated_model_uri = S3Uploader.upload(\n",
    "    \"model.tar.gz\", \"s3://{}/{}\".format(bucket, updated_pt_model_key)\n",
    ")\n",
    "\n",
    "# remove the new model.tar.gz from the current dir\n",
    "!rm model.tar.gz\n",
    "\n",
    "pt_updated_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229dd85",
   "metadata": {},
   "source": [
    "# Download the model.tar.gz containing the PyTorch model, to current dir\n",
    "S3Downloader.download(pt_mnist_model_data, \".\")\n",
    "\n",
    "# unzip the tar.gz\n",
    "!tar -xvf model.tar.gz\n",
    "\n",
    "# after unzipping, remove the model.tar.gz\n",
    "!rm model.tar.gz\n",
    "\n",
    "# copy the pytorch inference script to current dir\n",
    "!cp pytorch/code/inference.py .\n",
    "\n",
    "# gzip the inference.py and model file together in a new model.tar.gz\n",
    "!tar -czvf model.tar.gz model.pth inference.py\n",
    "\n",
    "# remove the residual files\n",
    "!rm inference.py model.pth\n",
    "\n",
    "# upload the new tar.gz to s3\n",
    "updated_pt_model_key = \"multi-container-endpoint/output/pytorch/updated\"\n",
    "pt_updated_model_uri = S3Uploader.upload(\n",
    "    \"model.tar.gz\", \"s3://{}/{}\".format(bucket, updated_pt_model_key)\n",
    ")\n",
    "\n",
    "# remove the new model.tar.gz from the current dir\n",
    "!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303b322",
   "metadata": {},
   "source": [
    "\n",
    "Now, everything is ready to create a container definition for PyTorch container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1d354c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_ecr_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.8.1\",\n",
    "    py_version=\"py36\",\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "pytorch_container = {\n",
    "    \"ContainerHostname\": \"pytorch-mnist\",\n",
    "    \"Image\": pt_ecr_image_uri,\n",
    "    \"ModelDataUrl\": pt_updated_model_uri,\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "        \"SAGEMAKER_SUBMIT_DIRECTORY\": pt_updated_model_uri,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84335532",
   "metadata": {},
   "source": [
    "### Create a SageMaker Model\n",
    "\n",
    "In the below cell, call the `create_model` API to create a model which contains the definitions of both the PyTorch and TensorFlow containers created above. It needs to supply both the containers under the `Containers` argument. Also set the `Mode` parameter of the `InferenceExecutionConfig` field to `Direct` for direct invocation of each container, or `Serial` to use containers as an inference pipeline. The default mode is `Serial`. For more details, check out [Deploy multi-container endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html)\n",
    "\n",
    "\n",
    "Since this notebook focuses on the Direct invocation behavior, hence set the value as `Direct`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc1edfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=\"mnist-multi-container\",\n",
    "    Containers=[pytorch_container, tensorflow_container],\n",
    "    InferenceExecutionConfig={\"Mode\": \"Direct\"},\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2514c2f",
   "metadata": {},
   "source": [
    "### Create Endpoint Configuration\n",
    "\n",
    "Now, create an endpoint configuration by calling the `create_endpoint_config` API. Here, supply the same `ModelName` used in the `create_model` API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e850513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=\"mnist-multi-container-ep-config\",\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"prod\",\n",
    "            \"ModelName\": \"mnist-multi-container\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": \"ml.c5.4xlarge\",\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55659b42",
   "metadata": {},
   "source": [
    "### Create a SageMaker Multi-container endpoint\n",
    "\n",
    "Now, the last step is to create a SageMaker multi-container endpoint. The `create_endpoint` API is used for this. The API behavior has no change compared to how a single container/model endpoint is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5374374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=\"mnist-multi-container-ep-2\", EndpointConfigName=\"mnist-multi-container-ep-config\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271681b7",
   "metadata": {},
   "source": [
    "The `create_endpoint` API is synchronous in nature and returns an immediate response with the endpoint status being in`Creating` state. It takes around ~8-10 minutes for multi-container endpoint to be `InService`.\n",
    "\n",
    "In the below cell, use the `describe_endpoint` API to check the status of endpoint creation. It runs a simple waiter loop calling the `describe_endpoint` API, for the endpoint to be `InService`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db23db52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current endpoint status is: Creating, Trying again...\n",
      "Current endpoint status is: Creating, Trying again...\n",
      "Current endpoint status is: Creating, Trying again...\n",
      "Current endpoint status is: Creating, Trying again...\n",
      "Current endpoint status is: Creating, Trying again...\n",
      "Current endpoint status is: Creating, Trying again...\n",
      "Current endpoint status is: Creating, Trying again...\n",
      "Endpoint status changed to 'InService'\n"
     ]
    }
   ],
   "source": [
    "describe_endpoint = sm_client.describe_endpoint(EndpointName=\"mnist-multi-container-ep-2\")\n",
    "\n",
    "endpoint_status = describe_endpoint[\"EndpointStatus\"]\n",
    "\n",
    "while endpoint_status != \"InService\":\n",
    "    print(\"Current endpoint status is: {}, Trying again...\".format(endpoint_status))\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=\"mnist-multi-container-ep-2\")\n",
    "    endpoint_status = resp[\"EndpointStatus\"]\n",
    "\n",
    "print(\"Endpoint status changed to 'InService'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66be9b",
   "metadata": {},
   "source": [
    "## Section 5: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fea86b",
   "metadata": {},
   "source": [
    "Now that the endpoint is set up it is time to perform inference on the endpoint by specifying one of the container host name. First, download the `MNIST` data and select a random sample of images. \n",
    "\n",
    "Use the helper functions defined in `code.utils` to download `MNIST` data set and normalize the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c972202c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x72 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "data_dir = \"data\"\n",
    "X, _ = mnist_to_numpy(data_dir, train=False)\n",
    "\n",
    "# randomly sample 16 images to inspect\n",
    "mask = random.sample(range(X.shape[0]), 16)\n",
    "samples = X[mask]\n",
    "\n",
    "# plot the images\n",
    "fig, axs = plt.subplots(nrows=1, ncols=16, figsize=(16, 1))\n",
    "\n",
    "for i, splt in enumerate(axs):\n",
    "    splt.imshow(samples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56ce68bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 28, 28) uint8\n"
     ]
    }
   ],
   "source": [
    "print(samples.shape, samples.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de4a788",
   "metadata": {},
   "source": [
    "### Invoking the TensorFlow container\n",
    "\n",
    "Now invoke the TensorFlow container, on the same endpoint. First normalize the sample selected and then pass the sample to the `invoke_endpoint` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daf76754",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_samples = normalize(samples, axis=(1, 2))\n",
    "\n",
    "tf_result = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=\"mnist-multi-container-ep\",\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"tensorflow-mnist\",\n",
    "    Body=json.dumps({\"instances\": np.expand_dims(tf_samples, 3).tolist()}),\n",
    ")\n",
    "\n",
    "tf_body = tf_result[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "tf_json_predictions = json.loads(tf_body)[\"predictions\"]\n",
    "\n",
    "# softmax to logit\n",
    "tf_predictions = np.array(tf_json_predictions, dtype=np.float32)\n",
    "tf_predictions = np.argmax(tf_json_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f254c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [1, 4, 1, 0, 9, 1, 9, 3, 1, 6, 2, 0, 6, 1, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions: \", tf_predictions.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83194ecf",
   "metadata": {},
   "source": [
    "### Invoke PyTorch container\n",
    "\n",
    "Now, invoke the PyTorch Container. In `transform_fn`, of `inference.py` it is declared that the parsed data is a python dictionary with a key `inputs` and its value should be a 1D array of length 784. Hence, create a sample inference data in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b700093",
   "metadata": {},
   "source": [
    "Before we invoke the SageMaker PyTorch model server with `samples`, we need to do some pre-processing\n",
    "- convert its data type to 32 bit floating point\n",
    "- normalize each channel (only one channel for `MNIST`)\n",
    "- add a channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5eb398c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digits:  [5, 3, 5, 3, 0, 3, 3, 5, 0, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "pt_samples = normalize(samples.astype(np.float32), axis=(1, 2))\n",
    "\n",
    "pt_result = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=\"mnist-multi-container-ep\",\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"pytorch-mnist\",\n",
    "    Body=json.dumps({\"inputs\": np.expand_dims(pt_samples, axis=1).tolist()}),\n",
    ")\n",
    "\n",
    "pt_body = pt_result[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "pt_predictions = np.argmax(np.array(json.loads(pt_body), dtype=np.float32), axis=1).tolist()\n",
    "print(\"Predicted digits: \", pt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68586b0b",
   "metadata": {},
   "source": [
    "\n",
    "## Section 6: clean up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe92bf8",
   "metadata": {},
   "source": [
    "Before leaving this exercise, it is a good practice to delete the resources created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm_client.delete_endpoint(EndpointName=\"mnist-multi-container-ep\")\n",
    "#sm_client.delete_endpoint_config(EndpointConfigName=\"mnist-multi-container-ep-config\")\n",
    "#sm_client.delete_model(ModelName=\"mnist-multi-container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d8a98",
   "metadata": {},
   "source": [
    "### This is the blog that provides more description of multi-container endpoing. It leverages this Jupyter notebook\n",
    "https://aws.amazon.com/blogs/machine-learning/deploy-multiple-serving-containers-on-a-single-instance-using-amazon-sagemaker-multi-container-endpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192fd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
